{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "48313b94",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import datasets, transforms\n",
    "import torch.optim as optim\n",
    "\n",
    "\n",
    "from training.train import train_model, train_protonet\n",
    "from utils.transforms import get_transforms\n",
    "from utils.evaluation import evaluate_model\n",
    "from models.ResNet_SC import ModifiedResNet18\n",
    "from models.ResNet import ResNet18, ResNet50\n",
    "from models.ProtoNet import ProtoNet18\n",
    "from models.FILM import FiLMNet18_SGC\n",
    "from datasets.Image_Classification import ImgClassificationDataset, CIFAR10C, FewShotDataset, load_cifar10"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f6a4362",
   "metadata": {},
   "source": [
    "train FiLM\n",
    "train Protonet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "dcd2ae45",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: mps\n"
     ]
    }
   ],
   "source": [
    "# Hyperparameters\n",
    "batch_size = 64\n",
    "num_epochs = 100\n",
    "lr = 0.0005\n",
    "num_classes = 10\n",
    "\n",
    "#loss_fn = \"ce\"\n",
    "use_koleo_loss = True\n",
    "\n",
    "CIFAR10C_ROOT = \"./data/cifar/CIFAR-10-C\"\n",
    "\n",
    "# List of all 15 corruption names in CIFAR-10-C (standard ordering)\n",
    "corruptions = [\n",
    "\t\"gaussian_noise\", \"shot_noise\", \"impulse_noise\",\n",
    "\t\"defocus_blur\", \"glass_blur\", \"motion_blur\", \"zoom_blur\",\n",
    "\t\"snow\", \"frost\", \"fog\", \"brightness\",\n",
    "\t\"contrast\", \"elastic_transform\", \"pixelate\",\n",
    "\t\"jpeg_compression\"\n",
    "]\n",
    "\n",
    "# Select device: prefer CUDA, then Apple MPS (for Apple Silicon), otherwise CPU\n",
    "if torch.cuda.is_available():\n",
    "\tdevice = torch.device(\"cuda\")\n",
    "elif hasattr(torch.backends, \"mps\") and torch.backends.mps.is_available():\n",
    "\tdevice = torch.device(\"mps\")\n",
    "else:\n",
    "\tdevice = torch.device(\"cpu\")\n",
    "\n",
    "print(f\"Using device: {device}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66bf5023",
   "metadata": {},
   "source": [
    "## 1) Load data\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "dd0121d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_transform = get_transforms(split='train')\n",
    "test_transform = get_transforms(split='test')\n",
    "\n",
    "\n",
    "\n",
    "train_dataset = datasets.CIFAR10(\n",
    "\troot=\"./data\",          \n",
    "\ttrain=True,             \n",
    "\tdownload=True,          \n",
    "\ttransform=train_transform\n",
    ")\n",
    "\n",
    "test_dataset = datasets.CIFAR10(\n",
    "\troot=\"./data\",\n",
    "\ttrain=False,\n",
    "\tdownload=True,\n",
    "\ttransform=test_transform\n",
    ")\n",
    "\n",
    "train_loader = DataLoader(\n",
    "\ttrain_dataset,\n",
    "\tbatch_size=batch_size,\n",
    "\tshuffle=True,      # shuffle training data each epoch\n",
    "\tnum_workers=2      # adjust number of workers to your machine\n",
    ")\n",
    "\n",
    "test_loader = DataLoader(\n",
    "\ttest_dataset,\n",
    "\tbatch_size=batch_size,\n",
    "\tshuffle=False,     # no need to shuffle test/validation data\n",
    "\tnum_workers=2\n",
    ")\n",
    "\n",
    "#############\n",
    "# Protonet dataset\n",
    "#############\n",
    "\n",
    "cifar10_train_imgs, cifar10_train_labels = load_cifar10(data_dir=\"./data/cifar-10-batches-py\", train=True, img_size=32)\n",
    "cifar10_test_imgs, cifar10_test_labels = load_cifar10(data_dir=\"./data/cifar-10-batches-py\", train=False, img_size=32)\n",
    "\n",
    "protonet_train_dataset = FewShotDataset(data=cifar10_train_imgs, labels=cifar10_train_labels, n_classes=7, n_supp=5, n_queries=15, transform=train_transform)\n",
    "protonet_test_dataset = FewShotDataset(data=cifar10_train_imgs, labels=cifar10_test_labels, n_classes=7, n_supp=5, n_queries=15, transform=train_transform)\n",
    "\n",
    "protonet_train_loader = DataLoader(protonet_train_dataset, batch_size=batch_size, shuffle=False, num_workers=0)\n",
    "protonet_test_loader = DataLoader(protonet_test_dataset, batch_size=batch_size, shuffle=False, num_workers=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f7034e4",
   "metadata": {},
   "source": [
    "## 2) Create model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7c25fa8d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ResNet18(\n",
      "  (conv1): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
      "  (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (relu): ReLU(inplace=True)\n",
      "  (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
      "  (layer1): Sequential(\n",
      "    (0): ResBlock18(\n",
      "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "    (1): ResBlock18(\n",
      "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "  )\n",
      "  (layer2): Sequential(\n",
      "    (0): ResBlock18(\n",
      "      (conv1): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (downsample): Sequential(\n",
      "        (0): Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
      "        (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (1): ResBlock18(\n",
      "      (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "  )\n",
      "  (layer3): Sequential(\n",
      "    (0): ResBlock18(\n",
      "      (conv1): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (downsample): Sequential(\n",
      "        (0): Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
      "        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (1): ResBlock18(\n",
      "      (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "  )\n",
      "  (layer4): Sequential(\n",
      "    (0): ResBlock18(\n",
      "      (conv1): Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (downsample): Sequential(\n",
      "        (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
      "        (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (1): ResBlock18(\n",
      "      (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "  )\n",
      "  (avgpool): AdaptiveAvgPool2d(output_size=(1, 1))\n",
      "  (fc): Linear(in_features=512, out_features=10, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "# (1) Plain ResNet-18 (no self-correction)\n",
    "\"\"\"\n",
    "model_plain = ModifiedResNet18(\n",
    "\tnum_classes=num_classes,\n",
    "\tuse_adabn=False,\n",
    "\tuse_cbam=False,\n",
    "\tuse_proto=False,\n",
    "    use_rbn=False,\n",
    ").to(device)\n",
    "\n",
    "\n",
    "# (2) ResNet-18 + AdaBN\n",
    "model_adabn = ModifiedResNet18(\n",
    "\tnum_classes=num_classes,\n",
    "\tuse_adabn=True,\n",
    "\tuse_cbam=False,\n",
    "\tuse_proto=False,\n",
    "\tuse_rbn=False,\n",
    ").to(device)\n",
    "\n",
    "# (3) ResNet-18 + CBAM\n",
    "model_cbam = ModifiedResNet18(\n",
    "\tnum_classes=num_classes,\n",
    "\tuse_adabn=False,\n",
    "\tuse_cbam=True,\n",
    "\tuse_proto=False,\n",
    "    use_rbn=False,\n",
    ").to(device)\n",
    "\n",
    "# (4) ResNet-18 + Prototype Alignment\n",
    "model_proto = ModifiedResNet18(\n",
    "\tnum_classes=num_classes,\n",
    "\tuse_adabn=False,\n",
    "\tuse_cbam=False,\n",
    "\tuse_proto=True,\n",
    "    use_rbn=False,\n",
    ").to(device)\n",
    "\"\"\"\n",
    "model_resnet18 = ResNet18(num_classes=10).to(device)\n",
    "model_resnet50 = ResNet50(num_classes=10).to(device)\n",
    "model_filmnet18_sgc = FiLMNet18_SGC(c_dim=512, num_classes=num_classes).to(device)\n",
    "model_protonet18 = ProtoNet18(ResNet18, embedding_dim=512, num_classes=num_classes).to(device)\n",
    "print(model_resnet18)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7b5363b",
   "metadata": {},
   "source": [
    "### 2.1) Model optimizers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2921447e",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "optim_resnet18  = optim.AdamW(model_resnet18.parameters(), lr=lr, weight_decay=5e-4)\n",
    "optim_resnet50  = optim.AdamW(model_resnet50.parameters(), lr=lr, weight_decay=5e-4)\n",
    "optim_protonet18 = optim.AdamW(model_protonet18.parameters(), lr=lr, weight_decay=5e-4)\n",
    "optim_filmnet18_sgc = optim.AdamW(model_filmnet18_sgc.parameters(), lr=lr, weight_decay=5e-4)\n",
    "optim_filmnet18_sgc = optim.AdamW(model_filmnet18_sgc.parameters(), lr=lr, weight_decay=5e-4)\n",
    "#optim_adabn  = optim.SGD(model_adabn.parameters(),  lr=lr, momentum=0.9, weight_decay=5e-4)\n",
    "#optim_cbam   = optim.SGD(model_cbam.parameters(),   lr=lr, momentum=0.9, weight_decay=5e-4)\n",
    "#optim_proto  = optim.SGD(model_proto.parameters(),  lr=lr, momentum=0.9, weight_decay=5e-4)\n",
    "\n",
    "# A dictionary to store (model, optimizer) pairs for easy looping:\n",
    "baseline_dict = {\n",
    "#    \"ResNet18\": (model_resnet18, optim_resnet18),\n",
    "#    \"ResNet50\": (model_resnet50, optim_resnet50),\n",
    "\t\"ProtoNet18\": (model_protonet18, optim_protonet18),\n",
    "#\t\"Proto\"   : (model_proto, optim_proto),\n",
    "#\t\"Plain\"   : (model_plain, optim_plain),\n",
    "#\t\"AdaBN\"   : (model_adabn, optim_adabn),\n",
    "#\t\"CBAM\"    : (model_cbam,  optim_cbam ),\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "484ed0d7",
   "metadata": {},
   "source": [
    "## 3) Train model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7acab78b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Training baseline: FilmNet18_SGC ===\n",
      "[FilmNet18_SGC][Epoch 1/100] train_loss=1.8157, train_acc=0.3296  val_loss=1.5656, val_acc=0.4486  (best=0.4486)  epoch_time=0h01m13s, ETA=2h01m19s\n",
      "[FilmNet18_SGC][Epoch 2/100] train_loss=1.5046, train_acc=0.4543  val_loss=1.4096, val_acc=0.5254  (best=0.5254)  epoch_time=0h01m09s, ETA=1h53m58s\n",
      "[FilmNet18_SGC][Epoch 3/100] train_loss=1.3758, train_acc=0.5040  val_loss=1.1598, val_acc=0.5981  (best=0.5981)  epoch_time=0h01m09s, ETA=1h52m22s\n",
      "[FilmNet18_SGC][Epoch 4/100] train_loss=1.2727, train_acc=0.5424  val_loss=1.1248, val_acc=0.6207  (best=0.6207)  epoch_time=0h01m09s, ETA=1h50m37s\n",
      "[FilmNet18_SGC][Epoch 5/100] train_loss=1.2133, train_acc=0.5678  val_loss=1.0678, val_acc=0.6472  (best=0.6472)  epoch_time=0h01m09s, ETA=1h49m38s\n",
      "[FilmNet18_SGC][Epoch 6/100] train_loss=1.1529, train_acc=0.5871  val_loss=1.0085, val_acc=0.6646  (best=0.6646)  epoch_time=0h01m09s, ETA=1h48m27s\n",
      "[FilmNet18_SGC][Epoch 7/100] train_loss=1.1100, train_acc=0.6063  val_loss=0.9909, val_acc=0.6693  (best=0.6693)  epoch_time=0h01m09s, ETA=1h47m15s\n",
      "[FilmNet18_SGC][Epoch 8/100] train_loss=1.0743, train_acc=0.6209  val_loss=0.9349, val_acc=0.6827  (best=0.6827)  epoch_time=0h01m09s, ETA=1h46m12s\n",
      "[FilmNet18_SGC][Epoch 9/100] train_loss=1.0515, train_acc=0.6295  val_loss=0.9139, val_acc=0.6946  (best=0.6946)  epoch_time=0h01m09s, ETA=1h44m54s\n",
      "[FilmNet18_SGC][Epoch 10/100] train_loss=1.0189, train_acc=0.6427  val_loss=0.9009, val_acc=0.7052  (best=0.7052)  epoch_time=0h01m09s, ETA=1h43m47s\n",
      "[FilmNet18_SGC][Epoch 11/100] train_loss=0.9896, train_acc=0.6491  val_loss=0.9449, val_acc=0.7004  (best=0.7052)  epoch_time=0h01m09s, ETA=1h42m47s\n",
      "[FilmNet18_SGC][Epoch 12/100] train_loss=0.9686, train_acc=0.6586  val_loss=0.8505, val_acc=0.7245  (best=0.7245)  epoch_time=0h01m07s, ETA=1h39m19s\n",
      "[FilmNet18_SGC][Epoch 13/100] train_loss=0.9543, train_acc=0.6635  val_loss=0.8904, val_acc=0.7180  (best=0.7245)  epoch_time=0h01m06s, ETA=1h36m50s\n",
      "[FilmNet18_SGC][Epoch 14/100] train_loss=0.9335, train_acc=0.6712  val_loss=0.8512, val_acc=0.7286  (best=0.7286)  epoch_time=0h01m06s, ETA=1h35m30s\n",
      "[FilmNet18_SGC][Epoch 15/100] train_loss=0.9188, train_acc=0.6762  val_loss=0.8532, val_acc=0.7236  (best=0.7286)  epoch_time=0h01m07s, ETA=1h35m06s\n",
      "[FilmNet18_SGC][Epoch 16/100] train_loss=0.9009, train_acc=0.6850  val_loss=0.8311, val_acc=0.7248  (best=0.7286)  epoch_time=0h01m07s, ETA=1h33m55s\n",
      "[FilmNet18_SGC][Epoch 17/100] train_loss=0.8855, train_acc=0.6889  val_loss=0.7874, val_acc=0.7398  (best=0.7398)  epoch_time=0h01m07s, ETA=1h32m49s\n",
      "[FilmNet18_SGC][Epoch 18/100] train_loss=0.8747, train_acc=0.6937  val_loss=0.7747, val_acc=0.7529  (best=0.7529)  epoch_time=0h01m06s, ETA=1h31m32s\n",
      "[FilmNet18_SGC][Epoch 19/100] train_loss=0.8603, train_acc=0.6967  val_loss=0.7884, val_acc=0.7313  (best=0.7529)  epoch_time=0h01m07s, ETA=1h30m48s\n",
      "[FilmNet18_SGC][Epoch 20/100] train_loss=0.8437, train_acc=0.7027  val_loss=0.7588, val_acc=0.7540  (best=0.7540)  epoch_time=0h01m07s, ETA=1h29m35s\n",
      "[FilmNet18_SGC][Epoch 21/100] train_loss=0.8383, train_acc=0.7065  val_loss=0.7393, val_acc=0.7555  (best=0.7555)  epoch_time=0h01m07s, ETA=1h29m11s\n",
      "[FilmNet18_SGC][Epoch 22/100] train_loss=0.8322, train_acc=0.7073  val_loss=0.7503, val_acc=0.7604  (best=0.7604)  epoch_time=0h01m08s, ETA=1h28m45s\n",
      "[FilmNet18_SGC][Epoch 23/100] train_loss=0.8174, train_acc=0.7133  val_loss=0.7020, val_acc=0.7661  (best=0.7661)  epoch_time=0h01m07s, ETA=1h27m13s\n",
      "[FilmNet18_SGC][Epoch 24/100] train_loss=0.8112, train_acc=0.7168  val_loss=0.7145, val_acc=0.7605  (best=0.7661)  epoch_time=0h01m07s, ETA=1h26m06s\n",
      "[FilmNet18_SGC][Epoch 25/100] train_loss=0.8010, train_acc=0.7161  val_loss=0.7482, val_acc=0.7462  (best=0.7661)  epoch_time=0h01m09s, ETA=1h26m42s\n",
      "[FilmNet18_SGC][Epoch 26/100] train_loss=0.7940, train_acc=0.7214  val_loss=0.7378, val_acc=0.7520  (best=0.7661)  epoch_time=0h01m09s, ETA=1h25m26s\n",
      "[FilmNet18_SGC][Epoch 27/100] train_loss=0.7861, train_acc=0.7246  val_loss=0.7268, val_acc=0.7553  (best=0.7661)  epoch_time=0h01m09s, ETA=1h24m19s\n",
      "[FilmNet18_SGC][Epoch 28/100] train_loss=0.7797, train_acc=0.7273  val_loss=0.7675, val_acc=0.7401  (best=0.7661)  epoch_time=0h01m11s, ETA=1h25m12s\n",
      "[FilmNet18_SGC][Epoch 29/100] train_loss=0.7673, train_acc=0.7305  val_loss=0.6969, val_acc=0.7646  (best=0.7661)  epoch_time=0h01m09s, ETA=1h22m39s\n",
      "[FilmNet18_SGC][Epoch 30/100] train_loss=0.7647, train_acc=0.7304  val_loss=0.6999, val_acc=0.7610  (best=0.7661)  epoch_time=0h01m11s, ETA=1h23m44s\n",
      "[FilmNet18_SGC][Epoch 31/100] train_loss=0.7549, train_acc=0.7351  val_loss=0.6828, val_acc=0.7668  (best=0.7668)  epoch_time=0h01m11s, ETA=1h21m52s\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[6]\u001b[39m\u001b[32m, line 12\u001b[39m\n\u001b[32m      2\u001b[39m best_val_acc = {name: \u001b[32m0.0\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m name \u001b[38;5;129;01min\u001b[39;00m baseline_dict.keys()}\n\u001b[32m      4\u001b[39m \u001b[38;5;66;03m#for name, (model, optim) in baseline_dict.items():\u001b[39;00m\n\u001b[32m      5\u001b[39m \u001b[38;5;66;03m#\ttrain_model(name=name, model=model, optimizer=optim, train_loader=train_loader, test_loader=test_loader, \u001b[39;00m\n\u001b[32m      6\u001b[39m \u001b[38;5;66;03m#\t\t\t\tcriterion=criterion, device=device, num_epochs=num_epochs)\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m      9\u001b[39m \u001b[38;5;66;03m#train_protonet(name=\"ProtoNet\", model=model_protonet18, optimizer=optim_protonet18, train_loader=protonet_train_loader, test_loader=protonet_test_loader,\u001b[39;00m\n\u001b[32m     10\u001b[39m \u001b[38;5;66;03m#\t\t\tcriterion=criterion, device=device, num_epochs=num_epochs)\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m12\u001b[39m \u001b[43mtrain_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mname\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mFilmNet18_SGC\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmodel_filmnet18_sgc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m=\u001b[49m\u001b[43moptim_filmnet18_sgc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtest_loader\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtest_loader\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     13\u001b[39m \u001b[43m\t\t\t\u001b[49m\u001b[43mcriterion\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcriterion\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_epochs\u001b[49m\u001b[43m=\u001b[49m\u001b[43mnum_epochs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/University/Edinburgh/Lightweight-SCNNs/training/train.py:50\u001b[39m, in \u001b[36mtrain_model\u001b[39m\u001b[34m(name, model, optimizer, train_loader, test_loader, criterion, device, num_epochs)\u001b[39m\n\u001b[32m     48\u001b[39m correct = \u001b[32m0\u001b[39m\n\u001b[32m     49\u001b[39m total = \u001b[32m0\u001b[39m\n\u001b[32m---> \u001b[39m\u001b[32m50\u001b[39m \u001b[43m\u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtargets\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m:\u001b[49m\n\u001b[32m     51\u001b[39m \u001b[43m\t\u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtargets\u001b[49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m.\u001b[49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtargets\u001b[49m\u001b[43m.\u001b[49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     52\u001b[39m \u001b[43m\t\u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m.\u001b[49m\u001b[43mzero_grad\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/installs/miniconda3/envs/SCNN/lib/python3.12/site-packages/torch/utils/data/dataloader.py:733\u001b[39m, in \u001b[36m_BaseDataLoaderIter.__next__\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    730\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    731\u001b[39m     \u001b[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[32m    732\u001b[39m     \u001b[38;5;28mself\u001b[39m._reset()  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m733\u001b[39m data = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_next_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    734\u001b[39m \u001b[38;5;28mself\u001b[39m._num_yielded += \u001b[32m1\u001b[39m\n\u001b[32m    735\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[32m    736\u001b[39m     \u001b[38;5;28mself\u001b[39m._dataset_kind == _DatasetKind.Iterable\n\u001b[32m    737\u001b[39m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m._IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    738\u001b[39m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m._num_yielded > \u001b[38;5;28mself\u001b[39m._IterableDataset_len_called\n\u001b[32m    739\u001b[39m ):\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/installs/miniconda3/envs/SCNN/lib/python3.12/site-packages/torch/utils/data/dataloader.py:1479\u001b[39m, in \u001b[36m_MultiProcessingDataLoaderIter._next_data\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m   1476\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m   1477\u001b[39m     \u001b[38;5;66;03m# no valid `self._rcvd_idx` is found (i.e., didn't break)\u001b[39;00m\n\u001b[32m   1478\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m._persistent_workers:\n\u001b[32m-> \u001b[39m\u001b[32m1479\u001b[39m         \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_shutdown_workers\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1480\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mStopIteration\u001b[39;00m\n\u001b[32m   1482\u001b[39m \u001b[38;5;66;03m# Now `self._rcvd_idx` is the batch index we want to fetch\u001b[39;00m\n\u001b[32m   1483\u001b[39m \n\u001b[32m   1484\u001b[39m \u001b[38;5;66;03m# Check if the next sample has already been generated\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/installs/miniconda3/envs/SCNN/lib/python3.12/site-packages/torch/utils/data/dataloader.py:1627\u001b[39m, in \u001b[36m_MultiProcessingDataLoaderIter._shutdown_workers\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m   1622\u001b[39m         \u001b[38;5;28mself\u001b[39m._mark_worker_as_unavailable(worker_id, shutdown=\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[32m   1623\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m w \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m._workers:\n\u001b[32m   1624\u001b[39m     \u001b[38;5;66;03m# We should be able to join here, but in case anything went\u001b[39;00m\n\u001b[32m   1625\u001b[39m     \u001b[38;5;66;03m# wrong, we set a timeout and if the workers fail to join,\u001b[39;00m\n\u001b[32m   1626\u001b[39m     \u001b[38;5;66;03m# they are killed in the `finally` block.\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m1627\u001b[39m     \u001b[43mw\u001b[49m\u001b[43m.\u001b[49m\u001b[43mjoin\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m=\u001b[49m\u001b[43m_utils\u001b[49m\u001b[43m.\u001b[49m\u001b[43mMP_STATUS_CHECK_INTERVAL\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1628\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m q \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m._index_queues:\n\u001b[32m   1629\u001b[39m     q.cancel_join_thread()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/installs/miniconda3/envs/SCNN/lib/python3.12/multiprocessing/process.py:149\u001b[39m, in \u001b[36mBaseProcess.join\u001b[39m\u001b[34m(self, timeout)\u001b[39m\n\u001b[32m    147\u001b[39m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m._parent_pid == os.getpid(), \u001b[33m'\u001b[39m\u001b[33mcan only join a child process\u001b[39m\u001b[33m'\u001b[39m\n\u001b[32m    148\u001b[39m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m._popen \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m, \u001b[33m'\u001b[39m\u001b[33mcan only join a started process\u001b[39m\u001b[33m'\u001b[39m\n\u001b[32m--> \u001b[39m\u001b[32m149\u001b[39m res = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_popen\u001b[49m\u001b[43m.\u001b[49m\u001b[43mwait\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    150\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m res \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    151\u001b[39m     _children.discard(\u001b[38;5;28mself\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/installs/miniconda3/envs/SCNN/lib/python3.12/multiprocessing/popen_fork.py:40\u001b[39m, in \u001b[36mPopen.wait\u001b[39m\u001b[34m(self, timeout)\u001b[39m\n\u001b[32m     38\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m timeout \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m     39\u001b[39m     \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mmultiprocessing\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mconnection\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m wait\n\u001b[32m---> \u001b[39m\u001b[32m40\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[43mwait\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43msentinel\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m:\n\u001b[32m     41\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m     42\u001b[39m \u001b[38;5;66;03m# This shouldn't block if wait() returned successfully.\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/installs/miniconda3/envs/SCNN/lib/python3.12/multiprocessing/connection.py:1136\u001b[39m, in \u001b[36mwait\u001b[39m\u001b[34m(object_list, timeout)\u001b[39m\n\u001b[32m   1133\u001b[39m     deadline = time.monotonic() + timeout\n\u001b[32m   1135\u001b[39m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1136\u001b[39m     ready = \u001b[43mselector\u001b[49m\u001b[43m.\u001b[49m\u001b[43mselect\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1137\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m ready:\n\u001b[32m   1138\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m [key.fileobj \u001b[38;5;28;01mfor\u001b[39;00m (key, events) \u001b[38;5;129;01min\u001b[39;00m ready]\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/installs/miniconda3/envs/SCNN/lib/python3.12/selectors.py:415\u001b[39m, in \u001b[36m_PollLikeSelector.select\u001b[39m\u001b[34m(self, timeout)\u001b[39m\n\u001b[32m    413\u001b[39m ready = []\n\u001b[32m    414\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m415\u001b[39m     fd_event_list = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_selector\u001b[49m\u001b[43m.\u001b[49m\u001b[43mpoll\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    416\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mInterruptedError\u001b[39;00m:\n\u001b[32m    417\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m ready\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "\n",
    "# Track best validation accuracy for each baseline:\n",
    "best_val_acc = {name: 0.0 for name in baseline_dict.keys()}\n",
    "\n",
    "#for name, (model, optim) in baseline_dict.items():\n",
    "#\ttrain_model(name=name, model=model, optimizer=optim, train_loader=train_loader, test_loader=test_loader, \n",
    "#\t\t\t\tcriterion=criterion, device=device, num_epochs=num_epochs)\n",
    "\n",
    "\n",
    "#train_protonet(name=\"ProtoNet\", model=model_protonet18, optimizer=optim_protonet18, train_loader=protonet_train_loader, test_loader=protonet_test_loader,\n",
    "#\t\t\tcriterion=criterion, device=device, num_epochs=num_epochs)\n",
    "\n",
    "train_model(name=\"FilmNet18_SGC\", model=model_filmnet18_sgc, optimizer=optim_filmnet18_sgc, train_loader=train_loader, test_loader=test_loader,\n",
    "\t\t\tcriterion=criterion, device=device, num_epochs=num_epochs)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e327993",
   "metadata": {},
   "source": [
    "### Save the models to disk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "530ba88f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n\\n# Dictionary mapping a human‐readable name → the actual nn.Module\\nmodels_to_save = {\\n\\t\"plain\":  model_plain,\\n\\t\"adabn\":  model_adabn,\\n\\t\"cbam\":   model_cbam,\\n\\t\"proto\":  model_proto,\\n}\\n\\n# Ensure the directory exists (in this case, we’re saving to the current working directory)\\nsave_dir = os.getcwd()\\nprint(f\"Saving models into: {save_dir}\")\\n\\nfor name, model in models_to_save.items():\\n\\t# Construct a filename like \"model_plain.pth\" or \"model_adabn.pth\"\\n\\tfilename = f\"model_{name}.pth\"\\n\\tfilepath = os.path.join(save_dir, filename)\\n\\n\\t# Save only the model’s state dict (weights + buffers)\\n\\ttorch.save(model.state_dict(), filepath)\\n\\tprint(f\"→ Saved {name} to {filename}\")\\n'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "\n",
    "# Dictionary mapping a human‐readable name → the actual nn.Module\n",
    "models_to_save = {\n",
    "\t\"plain\":  model_plain,\n",
    "\t\"adabn\":  model_adabn,\n",
    "\t\"cbam\":   model_cbam,\n",
    "\t\"proto\":  model_proto,\n",
    "}\n",
    "\n",
    "# Ensure the directory exists (in this case, we’re saving to the current working directory)\n",
    "save_dir = os.getcwd()\n",
    "print(f\"Saving models into: {save_dir}\")\n",
    "\n",
    "for name, model in models_to_save.items():\n",
    "\t# Construct a filename like \"model_plain.pth\" or \"model_adabn.pth\"\n",
    "\tfilename = f\"model_{name}.pth\"\n",
    "\tfilepath = os.path.join(save_dir, filename)\n",
    "\n",
    "\t# Save only the model’s state dict (weights + buffers)\n",
    "\ttorch.save(model.state_dict(), filepath)\n",
    "\tprint(f\"→ Saved {name} to {filename}\")\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef4abecd",
   "metadata": {},
   "source": [
    "### Load models from disk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "adcdbfaa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded Proto model from /Users/codiri/University/Edinburgh/Lightweight-SCNNs/model_Proto.pth\n",
      "Loaded Plain model from /Users/codiri/University/Edinburgh/Lightweight-SCNNs/model_Plain.pth\n",
      "Loaded AdaBN model from /Users/codiri/University/Edinburgh/Lightweight-SCNNs/model_AdaBN.pth\n",
      "Loaded CBAM model from /Users/codiri/University/Edinburgh/Lightweight-SCNNs/model_CBAM.pth\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Directory to save the models\n",
    "save_dir = os.getcwd()  # Current working directory\n",
    "\n",
    "\n",
    "for name, (model, optim) in baseline_dict.items():\n",
    "\tpath = os.path.join(save_dir, f\"model_{name}.pth\")\n",
    "\tif os.path.exists(path):\n",
    "\t\tstate_dict = torch.load(path, map_location=device)\n",
    "\t\tmodel.load_state_dict(state_dict)\n",
    "\t\tprint(f\"Loaded {name} model from {path}\")\n",
    "\telse:\n",
    "\t\tprint(f\"File not found: {path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1feced7b",
   "metadata": {},
   "source": [
    "## 4) Evaluate model on CIFAR-10C dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "771cc68d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Evaluating corruption: gaussian_noise (severity=1)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/codiri/installs/miniconda3/envs/SCNN/lib/python3.12/site-packages/torch/utils/data/dataloader.py:683: UserWarning: 'pin_memory' argument is set as true but not supported on MPS now, then device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Corruption = gaussian_noise  | Proto: 83.88%  Plain: 82.12%  AdaBN: 83.18%  CBAM: 72.01%\n",
      "\n",
      "Evaluating corruption: shot_noise (severity=1)\n",
      "Corruption = shot_noise      | Proto: 86.41%  Plain: 85.57%  AdaBN: 84.61%  CBAM: 79.24%\n",
      "\n",
      "Evaluating corruption: impulse_noise (severity=1)\n",
      "Corruption = impulse_noise   | Proto: 87.24%  Plain: 87.74%  AdaBN: 87.14%  CBAM: 87.12%\n",
      "\n",
      "Evaluating corruption: defocus_blur (severity=1)\n",
      "Corruption = defocus_blur    | Proto: 88.88%  Plain: 88.83%  AdaBN: 87.99%  CBAM: 87.73%\n",
      "\n",
      "Evaluating corruption: glass_blur (severity=1)\n",
      "Corruption = glass_blur      | Proto: 74.55%  Plain: 73.23%  AdaBN: 77.12%  CBAM: 66.27%\n",
      "\n",
      "Evaluating corruption: motion_blur (severity=1)\n",
      "Corruption = motion_blur     | Proto: 86.97%  Plain: 85.62%  AdaBN: 86.77%  CBAM: 79.99%\n",
      "\n",
      "Evaluating corruption: zoom_blur (severity=1)\n",
      "Corruption = zoom_blur       | Proto: 84.38%  Plain: 82.98%  AdaBN: 86.37%  CBAM: 77.85%\n",
      "\n",
      "Evaluating corruption: snow (severity=1)\n",
      "Corruption = snow            | Proto: 86.65%  Plain: 84.35%  AdaBN: 85.79%  CBAM: 85.39%\n",
      "\n",
      "Evaluating corruption: frost (severity=1)\n",
      "Corruption = frost           | Proto: 87.10%  Plain: 86.92%  AdaBN: 86.36%  CBAM: 83.47%\n",
      "\n",
      "Evaluating corruption: fog (severity=1)\n",
      "Corruption = fog             | Proto: 88.93%  Plain: 88.72%  AdaBN: 88.46%  CBAM: 87.77%\n",
      "\n",
      "Evaluating corruption: brightness (severity=1)\n",
      "Corruption = brightness      | Proto: 88.71%  Plain: 88.87%  AdaBN: 88.26%  CBAM: 88.11%\n",
      "\n",
      "Evaluating corruption: contrast (severity=1)\n",
      "Corruption = contrast        | Proto: 89.04%  Plain: 88.47%  AdaBN: 88.19%  CBAM: 87.31%\n",
      "\n",
      "Evaluating corruption: elastic_transform (severity=1)\n",
      "Corruption = elastic_transform | Proto: 85.52%  Plain: 84.36%  AdaBN: 84.85%  CBAM: 81.17%\n",
      "\n",
      "Evaluating corruption: pixelate (severity=1)\n",
      "Corruption = pixelate        | Proto: 87.57%  Plain: 86.78%  AdaBN: 86.86%  CBAM: 86.63%\n",
      "\n",
      "Evaluating corruption: jpeg_compression (severity=1)\n",
      "Corruption = jpeg_compression | Proto: 85.42%  Plain: 85.23%  AdaBN: 83.46%  CBAM: 82.84%\n",
      "\n",
      "Average accuracy over all 15 corruptions (severity=1):\n",
      "  Proto  →  86.08%\n",
      "  Plain  →  85.32%\n",
      "  AdaBN  →  85.69%\n",
      "  CBAM   →  82.19%\n"
     ]
    }
   ],
   "source": [
    "severity = 1  # Severity level (1-5), where 1 is the least severe\n",
    "batch_size = 256\n",
    "\n",
    "# We'll store per-model, per-corruption accuracies here:\n",
    "results = { name: [] for name in baseline_dict.keys() }\n",
    "\n",
    "# For each corruption type, build a CIFAR10C loader and measure accuracy:\n",
    "for corruption in corruptions:\n",
    "\t# Create the CIFAR-10-C dataset for this corruption & severity\n",
    "\tprint(f\"\\nEvaluating corruption: {corruption} (severity={severity})\")\n",
    "\tds_c = CIFAR10C(\n",
    "\t\tdata_dir=CIFAR10C_ROOT,\n",
    "\t\tcorruption=corruption,\n",
    "\t\tseverity=severity,\n",
    "\t\ttransform=test_transform\n",
    "\t)\n",
    "\tloader_c = DataLoader(ds_c,\n",
    "\t\t\t\t\t\t  batch_size=batch_size,\n",
    "\t\t\t\t\t\t  shuffle=False,\n",
    "\t\t\t\t\t\t  num_workers=2,\n",
    "\t\t\t\t\t\t  pin_memory=True)\n",
    "\n",
    "\t# For each model, run inference on this loader and compute accuracy\n",
    "\tfor name, (model, optim) in baseline_dict.items():\n",
    "\t\t#print(f\"  Evaluating {name}...\", end='\\n')\n",
    "\t\tmodel.eval()  # Set model to evaluation mode\n",
    "\t\tcorrect = 0\n",
    "\t\ttotal = 0\n",
    "\n",
    "\t\twith torch.no_grad():\n",
    "\t\t\tfor images, labels in loader_c:\n",
    "\t\t\t\timages = images.to(device)\n",
    "\t\t\t\tlabels = labels.to(device)\n",
    "\n",
    "\t\t\t\toutputs = model(images)               # (B, 10)\n",
    "\t\t\t\t_, preds = outputs.max(dim=1)         # (B,)\n",
    "\t\t\t\tcorrect += (preds == labels).sum().item()\n",
    "\t\t\t\ttotal += labels.size(0)\n",
    "\t\t\t\t#print(f\"  {name}: {correct}/{total} ({100 * correct / total:.2f}%)\", end='\\r')\n",
    "\n",
    "\t\tacc = correct / total\n",
    "\t\tresults[name].append(acc)\n",
    "\n",
    "\tprint(f\"Corruption = {corruption:<15} | \"\n",
    "\t\t  + \"  \".join([f\"{n}: {results[n][-1]*100:5.2f}%\" for n in baseline_dict.keys()]) )\n",
    "\n",
    "# 5) Summarize average across all corruptions\n",
    "# ─────────────────────────────────────────────────────────────────────────────\n",
    "\n",
    "print(\"\\nAverage accuracy over all 15 corruptions (severity=1):\")\n",
    "for name in baseline_dict.keys():\n",
    "\tavg_acc = np.mean(results[name])\n",
    "\tprint(f\"  {name:<5}  →  {avg_acc*100:5.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46281fc1",
   "metadata": {},
   "source": [
    "### 4.1) Load concatenated CIFAR-10C dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6aa1c44",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading CIFAR-10-C corruption arrays...\n",
      "Corruption: gaussian_noise, shape: (50000, 32, 32, 3), dtype: uint8\n",
      "Corruption: shot_noise, shape: (50000, 32, 32, 3), dtype: uint8\n",
      "Corruption: impulse_noise, shape: (50000, 32, 32, 3), dtype: uint8\n",
      "Corruption: defocus_blur, shape: (50000, 32, 32, 3), dtype: uint8\n",
      "Corruption: glass_blur, shape: (50000, 32, 32, 3), dtype: uint8\n",
      "Corruption: motion_blur, shape: (50000, 32, 32, 3), dtype: uint8\n",
      "Corruption: zoom_blur, shape: (50000, 32, 32, 3), dtype: uint8\n",
      "Corruption: snow, shape: (50000, 32, 32, 3), dtype: uint8\n",
      "Corruption: frost, shape: (50000, 32, 32, 3), dtype: uint8\n",
      "Corruption: fog, shape: (50000, 32, 32, 3), dtype: uint8\n",
      "Corruption: brightness, shape: (50000, 32, 32, 3), dtype: uint8\n",
      "Corruption: contrast, shape: (50000, 32, 32, 3), dtype: uint8\n",
      "Corruption: elastic_transform, shape: (50000, 32, 32, 3), dtype: uint8\n",
      "Corruption: pixelate, shape: (50000, 32, 32, 3), dtype: uint8\n",
      "Corruption: jpeg_compression, shape: (50000, 32, 32, 3), dtype: uint8\n",
      "Concatenated array shape: (750000, 32, 32, 3), dtype: uint8\n",
      "Labels shape: (50000,), dtype: uint8\n",
      "All labels shape: (750000,), dtype: uint8\n"
     ]
    }
   ],
   "source": [
    "# Load and inspect CIFAR-10-C corruption arrays\n",
    "print(\"Loading CIFAR-10-C corruption arrays...\")\n",
    "arr = []\n",
    "\n",
    "\n",
    "for corruption in corruptions:\n",
    "\tarr_c = np.load(os.path.join(CIFAR10C_ROOT, f\"{corruption}.npy\"))\n",
    "\tprint(f\"Corruption: {corruption}, shape: {arr_c.shape}, dtype: {arr_c.dtype}\")\n",
    "\n",
    "\t# concatanate all corruption arrays into a single tensor\n",
    "\tarr.append(arr_c)\n",
    "\n",
    "arr = np.concatenate(arr, axis=0)\n",
    "print(f\"Concatenated array shape: {arr.shape}, dtype: {arr.dtype}\")\n",
    "\n",
    "labels = np.load(os.path.join(CIFAR10C_ROOT, \"labels.npy\"))\n",
    "print(f\"Labels shape: {labels.shape}, dtype: {labels.dtype}\")\n",
    "all_labels = np.concatenate([labels] * len(corruptions), axis=0)\n",
    "print(f\"All labels shape: {all_labels.shape}, dtype: {all_labels.dtype}\")\n",
    "\n",
    "# Create a daatset from the concatenated array\n",
    "ds_cifar10c = ImgClassificationDataset(\n",
    "\tdata=arr,\n",
    "\tlabels=all_labels,\n",
    "\ttransform=test_transform\n",
    ")\n",
    "\n",
    "# Create a DataLoader for the CIFAR-10-C dataset\n",
    "loader_cifar10c = DataLoader(\n",
    "\tds_cifar10c,\n",
    "\tbatch_size=batch_size,\n",
    "\tshuffle=False,\n",
    "\tnum_workers=2\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1fa7179",
   "metadata": {},
   "source": [
    "### 4.2) Compute ECE over entire CIFAR-10-C dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4972af11",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Evaluating Proto on CIFAR-10-C dataset...\n",
      "Processed 515680 / 750000 images\n",
      "ECE over CIFAR-10-C: 4.99%\n",
      "Accuracy over CIFAR-10-C: 79.99%\n",
      "Proto - Accuracy: 79.99%, ECE: 0.0499\n",
      "\n",
      "Evaluating Plain on CIFAR-10-C dataset...\n",
      "Processed 515680 / 750000 images\n",
      "ECE over CIFAR-10-C: 10.21%\n",
      "Accuracy over CIFAR-10-C: 76.07%\n",
      "Plain - Accuracy: 76.07%, ECE: 0.1021\n",
      "\n",
      "Evaluating AdaBN on CIFAR-10-C dataset...\n",
      "Processed 515680 / 750000 images\n",
      "ECE over CIFAR-10-C: 5.40%\n",
      "Accuracy over CIFAR-10-C: 81.75%\n",
      "AdaBN - Accuracy: 81.75%, ECE: 0.0540\n",
      "\n",
      "Evaluating CBAM on CIFAR-10-C dataset...\n",
      "Processed 515680 / 750000 images\n",
      "ECE over CIFAR-10-C: 15.18%\n",
      "Accuracy over CIFAR-10-C: 70.39%\n",
      "CBAM - Accuracy: 70.39%, ECE: 0.1518\n"
     ]
    }
   ],
   "source": [
    "# compute accuracy and ECE for each model on the CIFAR-10-C dataset\n",
    "acc_list = []\n",
    "ece_list = []\n",
    "\n",
    "for name, (model, optim) in baseline_dict.items():\n",
    "\tprint(f\"\\nEvaluating {name} on CIFAR-10-C dataset...\")\n",
    "\tece, acc = evaluate_model(model=model,\n",
    "\t\t\t\t   loader=loader_cifar10c,\n",
    "\t\t\t\t   device=device)\n",
    "\tacc_list.append(acc)\n",
    "\tece_list.append(ece)\n",
    "\tprint(f\"{name} - Accuracy: {acc*100:.2f}%, ECE: {ece:.4f}\")\n",
    "\n",
    "\t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfbe7648",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "SCNN",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
