{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7618ef8c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/codiri/installs/miniconda3/envs/SCNN/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device: mps\n"
     ]
    }
   ],
   "source": [
    "import os, time, numpy as np\n",
    "from typing import List, Tuple, Dict\n",
    "\n",
    "import torch, torch.nn as nn, torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import datasets, transforms\n",
    "from PIL import Image\n",
    "import open_clip\n",
    "\n",
    "# Repro & device\n",
    "def set_seed(seed=42):\n",
    "\ttorch.manual_seed(seed); torch.cuda.manual_seed_all(seed); np.random.seed(seed)\n",
    "\n",
    "def device_auto():\n",
    "\tif torch.cuda.is_available(): return \"cuda\"\n",
    "\tif getattr(torch.backends, \"mps\", None) and torch.backends.mps.is_available(): return \"mps\"\n",
    "\treturn \"cpu\"\n",
    "\n",
    "device = device_auto()\n",
    "set_seed(42)\n",
    "print(\"Device:\", device)\n",
    "\n",
    "# Collate: keep PILs as a list (avoid default_collate error)\n",
    "def collate_src(batch):\n",
    "\timgs, y = zip(*batch)               # imgs: tuple of PIL\n",
    "\treturn list(imgs), torch.tensor(y, dtype=torch.long)\n",
    "\n",
    "def collate_imgs(batch):\n",
    "\treturn list(batch)                  # list of PIL\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13bea433",
   "metadata": {},
   "source": [
    "## 1) Config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3b274770",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Paths ---\n",
    "CIFAR10C_DIR = \"./data/cifar/CIFAR-10-C\"   # folder with *.npy + labels.npy (download from the official release)\n",
    "\n",
    "# --- Training hparams ---\n",
    "EPOCHS = 10          # try 10–30\n",
    "BATCH_SIZE = 128\n",
    "LR = 1e-6            # small LR (vision encoder only)\n",
    "WEIGHT_DECAY = 1e-4\n",
    "TAU = 0.07           # CLIP temperature (used on image–text sims)\n",
    "PL_CONF_THRESH = 0.4 # pseudo-label confidence threshold\n",
    "LAMBDA_DCP = 0.5     # weight on target debiased consistency\n",
    "L2_DELTA = 0.0       # (optional) L2 on correction—unused here\n",
    "\n",
    "# --- DataLoader settings ---\n",
    "NUM_WORKERS = 0 if device == \"mps\" else 2\n",
    "PIN_MEMORY  = (device == \"cuda\")\n",
    "\n",
    "# --- Domain-aware prompt words ---\n",
    "SRC_DOMAIN = \"natural\"\n",
    "TGT_DOMAIN = \"corrupted\"\n",
    "\n",
    "# CIFAR-10 classnames (order matches torchvision targets)\n",
    "classnames = [\"airplane\",\"automobile\",\"bird\",\"cat\",\"deer\",\"dog\",\"frog\",\"horse\",\"ship\",\"truck\"]\n",
    "\n",
    "# CIFAR-10-C corruption list\n",
    "CORRUPTIONS = [\n",
    "\t\"gaussian_noise\",\"shot_noise\",\"impulse_noise\",\"defocus_blur\",\"glass_blur\",\n",
    "\t\"motion_blur\",\"zoom_blur\",\"snow\",\"frost\",\"fog\",\"brightness\",\"contrast\",\n",
    "\t\"elastic_transform\",\"pixelate\",\"jpeg_compression\"\n",
    "]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d09661cb",
   "metadata": {},
   "source": [
    "## 2) Model and prompt encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "28fc48aa",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/codiri/installs/miniconda3/envs/SCNN/lib/python3.12/site-packages/open_clip/factory.py:450: UserWarning: QuickGELU mismatch between final model config (quick_gelu=False) and pretrained tag 'openai' (quick_gelu=True).\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# Load CLIP (ViT-B/32). Freeze text encoder.\n",
    "clip_model, _, clip_preprocess = open_clip.create_model_and_transforms(\"ViT-B-32\", pretrained=\"openai\")\n",
    "clip_model = clip_model.to(device)\n",
    "for p in clip_model.transformer.parameters():  # freeze text encoder\n",
    "\tp.requires_grad_(False)\n",
    "\n",
    "tokenizer = open_clip.get_tokenizer(\"ViT-B-32\")\n",
    "\n",
    "@torch.no_grad()\n",
    "def build_text_matrix(classnames: List[str], domain_word: str):\n",
    "\tprompts = [f\"a {domain_word} photo of a {c}\" for c in classnames]\n",
    "\ttok = tokenizer(prompts).to(device)\n",
    "\tt = clip_model.encode_text(tok)\n",
    "\treturn t / t.norm(dim=-1, keepdim=True)   # [C,d]\n",
    "\n",
    "E_text_src = build_text_matrix(classnames, SRC_DOMAIN)\n",
    "E_text_tgt = build_text_matrix(classnames, TGT_DOMAIN)\n",
    "\n",
    "# We’ll reuse CLIP’s mean/std for normalization after our augmentations\n",
    "MEAN = clip_preprocess.transforms[-1].mean\n",
    "STD  = clip_preprocess.transforms[-1].std\n",
    "to_clip = transforms.Compose([transforms.Resize(224), transforms.ToTensor(),\n",
    "\t\t\t\t\t\t\t  transforms.Normalize(mean=MEAN, std=STD)])\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0dc6bfc0",
   "metadata": {},
   "source": [
    "## 3) Datasets and Loaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c16c6e4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# CIFAR-10 source (train, labeled); CIFAR-10 test (clean eval)\n",
    "src_train_base = datasets.CIFAR10(root=\"./data\", train=True,  download=True)\n",
    "src_test_base  = datasets.CIFAR10(root=\"./data\", train=False, download=True)\n",
    "\n",
    "src_train_loader = DataLoader(src_train_base, batch_size=BATCH_SIZE, shuffle=True,\n",
    "\t\t\t\t\t\t\t  num_workers=NUM_WORKERS, pin_memory=PIN_MEMORY, collate_fn=collate_src)\n",
    "src_test_loader  = DataLoader(src_test_base,  batch_size=BATCH_SIZE, shuffle=False,\n",
    "\t\t\t\t\t\t\t  num_workers=NUM_WORKERS, pin_memory=PIN_MEMORY, collate_fn=collate_src)\n",
    "\n",
    "# CIFAR-10-C unlabeled target (all corruptions, severities 1–5) — return ONLY PIL images\n",
    "class CIFAR10C_Unlabeled(Dataset):\n",
    "\tdef __init__(self, root, corruptions, severities=(1,2,3,4,5)):\n",
    "\t\tself.root = root\n",
    "\t\tself.corruptions = list(corruptions)\n",
    "\t\tself.severities = list(severities)\n",
    "\t\t# build (corr, sev, idx) index\n",
    "\t\tself.index = []\n",
    "\t\t# length per severity per corruption is 10k (based on standard release)\n",
    "\t\tfor c in self.corruptions:\n",
    "\t\t\tx = np.load(os.path.join(root, f\"{c}.npy\"))  # shape [50000, 32, 32, 3]\n",
    "\t\t\tfor s in self.severities:\n",
    "\t\t\t\tstart = (s-1)*10000; end = s*10000\n",
    "\t\t\t\tlength = end - start\n",
    "\t\t\t\tself.index.extend([(c, s, i) for i in range(start, end)])\n",
    "\t\t\t# cache array handle for quick access\n",
    "\t\tself._cache = {}  # corruption -> numpy array\n",
    "\tdef __len__(self): return len(self.index)\n",
    "\tdef __getitem__(self, i):\n",
    "\t\tc, s, idx = self.index[i]\n",
    "\t\tif c not in self._cache:\n",
    "\t\t\tself._cache[c] = np.load(os.path.join(self.root, f\"{c}.npy\"))\n",
    "\t\tarr = self._cache[c][idx]  # [32,32,3] uint8\n",
    "\t\treturn Image.fromarray(arr)\n",
    "\n",
    "tgt_train_base = CIFAR10C_Unlabeled(CIFAR10C_DIR, CORRUPTIONS, severities=(1,2,3,4,5))\n",
    "tgt_train_loader = DataLoader(tgt_train_base, batch_size=BATCH_SIZE, shuffle=True,\n",
    "\t\t\t\t\t\t\t  num_workers=NUM_WORKERS, pin_memory=PIN_MEMORY, collate_fn=collate_imgs)\n",
    "\n",
    "# For evaluation on CIFAR-10-C with labels\n",
    "class CIFAR10C_Labeled(Dataset):\n",
    "\tdef __init__(self, root, corruption, severity):\n",
    "\t\tself.X = np.load(os.path.join(root, f\"{corruption}.npy\"))     # [50000,32,32,3]\n",
    "\t\tself.Y = np.load(os.path.join(root, \"labels.npy\")).astype(int) # [50000]\n",
    "\t\tstart = (severity-1)*10000; end = severity*10000\n",
    "\t\tself.X = self.X[start:end]; self.Y = self.Y[start:end]\n",
    "\tdef __len__(self): return len(self.Y)\n",
    "\tdef __getitem__(self, i):\n",
    "\t\treturn Image.fromarray(self.X[i]), int(self.Y[i])\n",
    "\n",
    "def make_c10c_eval_loader(corr, sev, bs=BATCH_SIZE):\n",
    "\tds = CIFAR10C_Labeled(CIFAR10C_DIR, corr, sev)\n",
    "\treturn DataLoader(ds, batch_size=bs, shuffle=False,\n",
    "\t\t\t\t\t  num_workers=NUM_WORKERS, pin_memory=PIN_MEMORY, collate_fn=collate_src)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e91f6c0",
   "metadata": {},
   "source": [
    "## 4) Augmentations (weak/strong)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "30269a42",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Weak/Strong views (then CLIP-normalize)\n",
    "aug_weak = transforms.Compose([\n",
    "\ttransforms.RandomResizedCrop(224, scale=(0.8,1.0)),\n",
    "\ttransforms.RandomHorizontalFlip(p=0.5),\n",
    "])\n",
    "aug_strong = transforms.Compose([\n",
    "\ttransforms.RandomResizedCrop(224, scale=(0.5,1.0)),\n",
    "\ttransforms.RandAugment(num_ops=2, magnitude=9),\n",
    "\ttransforms.RandomHorizontalFlip(p=0.5),\n",
    "])\n",
    "\n",
    "@torch.no_grad()\n",
    "def encode_images(px):\n",
    "\tz = clip_model.encode_image(px)\n",
    "\treturn z / z.norm(dim=-1, keepdim=True)\n",
    "\n",
    "def logits_from_image_and_text(z_img, E_text):\n",
    "\tlogit_scale = clip_model.logit_scale.exp()\n",
    "\treturn logit_scale * (z_img @ E_text.t())\n",
    "\n",
    "def symmetric_clip_loss(z_img, z_txt):\n",
    "\tlogit_scale = clip_model.logit_scale.exp()\n",
    "\tsim_i2t = logit_scale * (z_img @ z_txt.t())\n",
    "\tsim_t2i = sim_i2t.t()\n",
    "\ttargets = torch.arange(z_img.size(0), device=z_img.device)\n",
    "\treturn 0.5*(F.cross_entropy(sim_i2t, targets) + F.cross_entropy(sim_t2i, targets))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59460d61",
   "metadata": {},
   "source": [
    "## 5) CFM/DCM (source & target) and PAD debiasor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6d134680",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cfm_and_dcm_for_batch(imgs_PIL):\n",
    "\t\"\"\"\n",
    "\tReturns:\n",
    "\t  lambda_cfm (float in [0,1]), momentum_dcm (float), z_w, z_s, z_o\n",
    "\t\"\"\"\n",
    "\t# Build three views\n",
    "\tpx_w = torch.stack([to_clip(aug_weak(img))  for img in imgs_PIL]).to(device)\n",
    "\tpx_s = torch.stack([to_clip(aug_strong(img)) for img in imgs_PIL]).to(device)\n",
    "\tpx_o = torch.stack([to_clip(transforms.Resize(224)(img)) for img in imgs_PIL]).to(device)\n",
    "\n",
    "\twith torch.no_grad():\n",
    "\t\tz_o = encode_images(px_o)\n",
    "\tz_w = encode_images(px_w)\n",
    "\tz_s = encode_images(px_s)\n",
    "\n",
    "\td_wo = (z_w - z_o).pow(2).sum(dim=-1)\n",
    "\td_so = (z_s - z_o).pow(2).sum(dim=-1)\n",
    "\td_ws = (z_w - z_s).pow(2).sum(dim=-1)\n",
    "\n",
    "\t# Simple bounded proxy for \"forgetting\"\n",
    "\tlam = 1.0 - ((d_wo + d_so + d_ws).mean() / 6.0) * 2.0\n",
    "\tlam = float(torch.clamp(lam, 0.0, 1.0))\n",
    "\n",
    "\tm = float(F.cosine_similarity(z_w, z_s, dim=-1).mean().clamp(-1,1))\n",
    "\treturn lam, m, z_w, z_s, z_o\n",
    "\n",
    "class PseudoLabelDebiasor:\n",
    "\tdef __init__(self, num_classes):\n",
    "\t\tself.C = num_classes\n",
    "\t\tself.p_prime = torch.full((self.C,), 1.0/self.C, device=device)\n",
    "\tdef step(self, probs_batch, m_t):\n",
    "\t\tbatch_mean = probs_batch.mean(dim=0).detach()\n",
    "\t\tself.p_prime = (m_t * self.p_prime + (1.0 - m_t) * batch_mean).clamp_(1e-6, 1.0)\n",
    "\t\tself.p_prime = self.p_prime / self.p_prime.sum()\n",
    "\tdef debias(self, q, lambda_t):\n",
    "\t\tq_prime = q - lambda_t * (self.p_prime + 1e-6).log().unsqueeze(0)\n",
    "\t\treturn F.softmax(q_prime, dim=-1)  # renormalize\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9f73a48",
   "metadata": {},
   "source": [
    "## 6) Training loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1234744c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train only the vision encoder (keep text frozen)\n",
    "params = [p for n,p in clip_model.named_parameters() if p.requires_grad and \"transformer\" not in n]\n",
    "opt = torch.optim.AdamW(params, lr=LR, weight_decay=WEIGHT_DECAY)\n",
    "sched = torch.optim.lr_scheduler.CosineAnnealingLR(opt, T_max=EPOCHS*max(1, len(src_train_loader)))\n",
    "\n",
    "lambda_s_run = 0.0\n",
    "lambda_t_run = 0.0\n",
    "pad = PseudoLabelDebiasor(num_classes=10)\n",
    "\n",
    "@torch.no_grad()\n",
    "def zero_shot_probs(z_img, E_text):\n",
    "\tlogits = logits_from_image_and_text(z_img, E_text)\n",
    "\treturn F.softmax(logits, dim=-1)\n",
    "\n",
    "def train_epoch():\n",
    "\tglobal lambda_s_run, lambda_t_run\n",
    "\tclip_model.train()\n",
    "\tit_src = iter(src_train_loader)\n",
    "\tit_tgt = iter(tgt_train_loader)\n",
    "\tsteps = min(len(src_train_loader), len(tgt_train_loader))\n",
    "\tsup_meter, unsup_meter = 0.0, 0.0\n",
    "\tprint()\n",
    "\tfor st in range(steps):\n",
    "\t\tprint(f\"Step {st+1}/{steps}\", end=\"\\r\")\n",
    "\t\t# ===== Source (labeled CIFAR-10) =====\n",
    "\t\ttry:\n",
    "\t\t\timgs_s, y = next(it_src)\n",
    "\t\texcept StopIteration:\n",
    "\t\t\tit_src = iter(src_train_loader); imgs_s, y = next(it_src)\n",
    "\t\ty = y.to(device)\n",
    "\n",
    "\t\tlam_s, m_s, z_w_s, z_s_s, z_o_s = cfm_and_dcm_for_batch(imgs_s)\n",
    "\t\tlambda_s_run = m_s * lam_s + (1 - m_s) * lambda_s_run\n",
    "\t\tlam_sup = float(lambda_s_run)\n",
    "\n",
    "\t\t# Per-sample text embeddings for gold labels (domain-aware)\n",
    "\t\twith torch.no_grad():\n",
    "\t\t\ttxt_prompts = [f\"a {SRC_DOMAIN} photo of a {classnames[int(yi)]}\" for yi in y]\n",
    "\t\t\ttok = tokenizer(txt_prompts).to(device)\n",
    "\t\t\tz_txt = clip_model.encode_text(tok); z_txt = z_txt / z_txt.norm(dim=-1, keepdim=True)\n",
    "\t\t# supervised uses weak-view images\n",
    "\t\tpx_sup = torch.stack([to_clip(aug_weak(img)) for img in imgs_s]).to(device)\n",
    "\t\tz_img_sup = encode_images(px_sup)\n",
    "\t\tL_sup = symmetric_clip_loss(z_img_sup, z_txt)\n",
    "\n",
    "\t\t# ===== Target (unlabeled CIFAR-10-C) =====\n",
    "\t\ttry:\n",
    "\t\t\timgs_t = next(it_tgt)\n",
    "\t\texcept StopIteration:\n",
    "\t\t\tit_tgt = iter(tgt_train_loader); imgs_t = next(it_tgt)\n",
    "\n",
    "\t\tlam_t, m_t, z_w_t, z_s_t, z_o_t = cfm_and_dcm_for_batch(imgs_t)\n",
    "\t\tlambda_t_run = m_t * lam_t + (1 - m_t) * lambda_t_run\n",
    "\t\tlam_debias = float(lambda_t_run)\n",
    "\n",
    "\t\tq_weak = zero_shot_probs(z_w_t, E_text_tgt)  # [B,10]\n",
    "\t\tpad.step(q_weak, m_t)                        # update running class prior\n",
    "\t\tq_deb = pad.debias(q_weak, lam_debias)       # debiased soft labels\n",
    "\n",
    "\t\t# consistency on strong-view predictions (masked by confidence)\n",
    "\t\tlogits_strong = logits_from_image_and_text(z_s_t, E_text_tgt)\n",
    "\t\tlogp_strong = F.log_softmax(logits_strong, dim=-1)\n",
    "\t\tconf = q_deb.max(dim=1).values\n",
    "\t\tmask = (conf >= PL_CONF_THRESH).float()\n",
    "\t\tif mask.sum() > 0:\n",
    "\t\t\tloss_vec = -(q_deb * logp_strong).sum(dim=1)\n",
    "\t\t\tL_dcp = (loss_vec * mask).sum() / (mask.sum() + 1e-6)\n",
    "\t\telse:\n",
    "\t\t\tL_dcp = torch.tensor(0.0, device=device)\n",
    "\n",
    "\t\t# ===== Combine & step =====\n",
    "\t\tloss = lam_sup * L_sup + LAMBDA_DCP * L_dcp\n",
    "\t\topt.zero_grad(set_to_none=True)\n",
    "\t\tloss.backward()\n",
    "\t\topt.step(); sched.step()\n",
    "\n",
    "\t\tsup_meter  += L_sup.item()\n",
    "\t\tunsup_meter += L_dcp.item()\n",
    "\n",
    "\tprint()\n",
    "\n",
    "\treturn sup_meter/steps, unsup_meter/steps\n",
    "\n",
    "# (Optional) zero-shot baselines before training\n",
    "@torch.no_grad()\n",
    "def eval_clean(loader):\n",
    "\tclip_model.eval(); correct=total=0\n",
    "\tfor imgs, y in loader:\n",
    "\t\ty = y.to(device)\n",
    "\t\tpx = torch.stack([to_clip(img) for img in imgs]).to(device)\n",
    "\t\tz  = encode_images(px)\n",
    "\t\tlogits = logits_from_image_and_text(z, E_text_src)  # clean domain prompts\n",
    "\t\tpred = logits.argmax(1)\n",
    "\t\tcorrect += (pred==y).sum().item(); total += y.size(0)\n",
    "\treturn correct/max(total,1)\n",
    "\n",
    "@torch.no_grad()\n",
    "def eval_c10c_mean():\n",
    "\tclip_model.eval(); accs=[]\n",
    "\tfor corr in CORRUPTIONS:\n",
    "\t\tprint(f\"Evaluating {corr}...\")\n",
    "\t\tcorr_acc=[]\n",
    "\t\tfor sev in [1,2,3,4,5]:\n",
    "\t\t\tprint(f\"Evaluating {corr} at severity {sev}...\")\n",
    "\t\t\tloader = make_c10c_eval_loader(corr, sev)\n",
    "\t\t\tcorrect=total=0\n",
    "\t\t\tfor imgs, y in loader:\n",
    "\t\t\t\ty = y.to(device)\n",
    "\t\t\t\tpx = torch.stack([to_clip(img) for img in imgs]).to(device)\n",
    "\t\t\t\tz  = encode_images(px)\n",
    "\t\t\t\tlogits = logits_from_image_and_text(z, E_text_tgt)  # target domain prompts\n",
    "\t\t\t\tpred = logits.argmax(1)\n",
    "\t\t\t\tcorrect += (pred==y).sum().item(); total += y.size(0)\n",
    "\t\t\tcorr_acc.append(correct/max(total,1))\n",
    "\t\taccs.append(np.mean(corr_acc))\n",
    "\treturn float(np.mean(accs))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8f52598",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Step 88/391\r"
     ]
    }
   ],
   "source": [
    "#print(\"Zero-shot (pre-train) — CIFAR-10 clean acc:\",  eval_clean(src_test_loader))\n",
    "#print(\"Zero-shot (pre-train) — CIFAR-10-C mean acc:\", eval_c10c_mean())\n",
    "\n",
    "for ep in range(1, EPOCHS+1):\n",
    "\tLs, Lt = train_epoch()\n",
    "\tprint(f\"[Epoch {ep:02d}] L_sup={Ls:.4f}  L_dcp={Lt:.4f}\")\n",
    "\n",
    "print(\"Adapted — CIFAR-10 clean acc:\",  eval_clean(src_test_loader))\n",
    "print(\"Adapted — CIFAR-10-C mean acc:\", eval_c10c_mean())\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "SCNN",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
