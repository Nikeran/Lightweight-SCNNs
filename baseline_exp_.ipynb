{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "48313b94",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import datasets, transforms\n",
    "import torch.optim as optim\n",
    "\n",
    "\n",
    "from training.train import train_model\n",
    "from utils.transforms import get_transforms\n",
    "from utils.evaluation import evaluate_model\n",
    "from models.ResNet_SC import ModifiedResNet18\n",
    "from models.ResNet import ResNet18\n",
    "from dataset.Image_Classification import ImgClassificationDataset, CIFAR10C"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dcd2ae45",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: mps\n"
     ]
    }
   ],
   "source": [
    "# Hyperparameters\n",
    "batch_size = 64\n",
    "num_epochs = 100\n",
    "lr = 0.0005\n",
    "num_classes = 10\n",
    "\n",
    "#loss_fn = \"ce\"\n",
    "use_koleo_loss = True\n",
    "\n",
    "CIFAR10C_ROOT = \"./data/cifar/CIFAR-10-C\"\n",
    "\n",
    "# List of all 15 corruption names in CIFAR-10-C (standard ordering)\n",
    "corruptions = [\n",
    "\t\"gaussian_noise\", \"shot_noise\", \"impulse_noise\",\n",
    "\t\"defocus_blur\", \"glass_blur\", \"motion_blur\", \"zoom_blur\",\n",
    "\t\"snow\", \"frost\", \"fog\", \"brightness\",\n",
    "\t\"contrast\", \"elastic_transform\", \"pixelate\",\n",
    "\t\"jpeg_compression\"\n",
    "]\n",
    "\n",
    "# Select device: prefer CUDA, then Apple MPS (for Apple Silicon), otherwise CPU\n",
    "if torch.cuda.is_available():\n",
    "\tdevice = torch.device(\"cuda\")\n",
    "elif hasattr(torch.backends, \"mps\") and torch.backends.mps.is_available():\n",
    "\tdevice = torch.device(\"mps\")\n",
    "else:\n",
    "\tdevice = torch.device(\"cpu\")\n",
    "\n",
    "print(f\"Using device: {device}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66bf5023",
   "metadata": {},
   "source": [
    "## 1) Load data\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd0121d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_transform = get_transforms(split='train')\n",
    "test_transform = get_transforms(split='test')\n",
    "\n",
    "\n",
    "\n",
    "train_dataset = datasets.CIFAR10(\n",
    "\troot=\"./data\",          \n",
    "\ttrain=True,             \n",
    "\tdownload=True,          \n",
    "\ttransform=train_transform\n",
    ")\n",
    "\n",
    "test_dataset = datasets.CIFAR10(\n",
    "\troot=\"./data\",\n",
    "\ttrain=False,\n",
    "\tdownload=True,\n",
    "\ttransform=test_transform\n",
    ")\n",
    "\n",
    "train_loader = DataLoader(\n",
    "\ttrain_dataset,\n",
    "\tbatch_size=batch_size,\n",
    "\tshuffle=True,      # shuffle training data each epoch\n",
    "\tnum_workers=2      # adjust number of workers to your machine\n",
    ")\n",
    "\n",
    "test_loader = DataLoader(\n",
    "\ttest_dataset,\n",
    "\tbatch_size=batch_size,\n",
    "\tshuffle=False,     # no need to shuffle test/validation data\n",
    "\tnum_workers=2\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f7034e4",
   "metadata": {},
   "source": [
    "## 2) Create model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c25fa8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# (1) Plain ResNet-18 (no self-correction)\n",
    "model_plain = ResNet18(\n",
    "\tnum_classes=num_classes,\n",
    "\tuse_adabn=False,\n",
    "\tuse_cbam=False,\n",
    "\tuse_proto=False,\n",
    "\tuse_rbn=False,\n",
    ").to(device)\n",
    "\n",
    "\n",
    "# (2) ResNet-18 + AdaBN\n",
    "model_adabn = ResNet18(\n",
    "\tnum_classes=num_classes,\n",
    "\tuse_adabn=True,\n",
    "\tuse_cbam=False,\n",
    "\tuse_proto=False,\n",
    "\tuse_rbn=False,\n",
    ").to(device)\n",
    "\n",
    "# (3) ResNet-18 + CBAM\n",
    "model_cbam = ResNet18(\n",
    "\tnum_classes=num_classes,\n",
    "\tuse_adabn=False,\n",
    "\tuse_cbam=True,\n",
    "\tuse_proto=False,\n",
    "\tuse_rbn=False,\n",
    ").to(device)\n",
    "\n",
    "# (4) ResNet-18 + Prototype Alignment\n",
    "model_proto = ResNet18(\n",
    "\tnum_classes=num_classes,\n",
    "\tuse_adabn=False,\n",
    "\tuse_cbam=False,\n",
    "\tuse_proto=True,\n",
    "\tuse_rbn=False,\n",
    ").to(device)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7b5363b",
   "metadata": {},
   "source": [
    "### 2.1) Model optimizers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2921447e",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "optim_plain  = optim.SGD(model_plain.parameters(),  lr=lr, momentum=0.9, weight_decay=5e-4)\n",
    "optim_adabn  = optim.SGD(model_adabn.parameters(),  lr=lr, momentum=0.9, weight_decay=5e-4)\n",
    "optim_cbam   = optim.SGD(model_cbam.parameters(),   lr=lr, momentum=0.9, weight_decay=5e-4)\n",
    "optim_proto  = optim.SGD(model_proto.parameters(),  lr=lr, momentum=0.9, weight_decay=5e-4)\n",
    "\n",
    "# A dictionary to store (model, optimizer) pairs for easy looping:\n",
    "baseline_dict = {\n",
    "\t\"Proto\"   : (model_proto, optim_proto),\n",
    "\t\"Plain\"   : (model_plain, optim_plain),\n",
    "\t\"AdaBN\"   : (model_adabn, optim_adabn),\n",
    "\t\"CBAM\"    : (model_cbam,  optim_cbam ),\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "484ed0d7",
   "metadata": {},
   "source": [
    "## 3) Train model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7acab78b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "# Track best validation accuracy for each baseline:\n",
    "best_val_acc = {name: 0.0 for name in baseline_dict.keys()}\n",
    "\n",
    "for name, (model, optim) in baseline_dict.items():\n",
    "\ttrain_model(name=name, model=model, optimizer=optim, train_loader=train_loader, test_loader=test_loader, \n",
    "\t\t\t\tcriterion=criterion, device=device, num_epochs=num_epochs)\n",
    "\n",
    "\n",
    "#train_model(name=\"ada\", model=model_adabn, optimizer=optim_adabn, train_loader=train_loader, test_loader=test_loader,\n",
    "#\t\t\tcriterion=criterion, device=device, num_epochs=num_epochs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e327993",
   "metadata": {},
   "source": [
    "### Save the models to disk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "530ba88f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n\\n# Dictionary mapping a human‐readable name → the actual nn.Module\\nmodels_to_save = {\\n\\t\"plain\":  model_plain,\\n\\t\"adabn\":  model_adabn,\\n\\t\"cbam\":   model_cbam,\\n\\t\"proto\":  model_proto,\\n}\\n\\n# Ensure the directory exists (in this case, we’re saving to the current working directory)\\nsave_dir = os.getcwd()\\nprint(f\"Saving models into: {save_dir}\")\\n\\nfor name, model in models_to_save.items():\\n\\t# Construct a filename like \"model_plain.pth\" or \"model_adabn.pth\"\\n\\tfilename = f\"model_{name}.pth\"\\n\\tfilepath = os.path.join(save_dir, filename)\\n\\n\\t# Save only the model’s state dict (weights + buffers)\\n\\ttorch.save(model.state_dict(), filepath)\\n\\tprint(f\"→ Saved {name} to {filename}\")\\n'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "\n",
    "# Dictionary mapping a human‐readable name → the actual nn.Module\n",
    "models_to_save = {\n",
    "\t\"plain\":  model_plain,\n",
    "\t\"adabn\":  model_adabn,\n",
    "\t\"cbam\":   model_cbam,\n",
    "\t\"proto\":  model_proto,\n",
    "}\n",
    "\n",
    "# Ensure the directory exists (in this case, we’re saving to the current working directory)\n",
    "save_dir = os.getcwd()\n",
    "print(f\"Saving models into: {save_dir}\")\n",
    "\n",
    "for name, model in models_to_save.items():\n",
    "\t# Construct a filename like \"model_plain.pth\" or \"model_adabn.pth\"\n",
    "\tfilename = f\"model_{name}.pth\"\n",
    "\tfilepath = os.path.join(save_dir, filename)\n",
    "\n",
    "\t# Save only the model’s state dict (weights + buffers)\n",
    "\ttorch.save(model.state_dict(), filepath)\n",
    "\tprint(f\"→ Saved {name} to {filename}\")\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef4abecd",
   "metadata": {},
   "source": [
    "### Load models from disk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "adcdbfaa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading model: from /Users/codiri/University/Edinburgh/Lightweight-SCNNs/ckpt_epoch_150_Proto.pt\n",
      "Loaded Proto model from /Users/codiri/University/Edinburgh/Lightweight-SCNNs/ckpt_epoch_150_Proto.pt\n",
      "Loading model: from /Users/codiri/University/Edinburgh/Lightweight-SCNNs/ckpt_epoch_150_Plain.pt\n",
      "Loaded Plain model from /Users/codiri/University/Edinburgh/Lightweight-SCNNs/ckpt_epoch_150_Plain.pt\n",
      "Loading model: from /Users/codiri/University/Edinburgh/Lightweight-SCNNs/ckpt_epoch_150_AdaBN.pt\n",
      "Loaded AdaBN model from /Users/codiri/University/Edinburgh/Lightweight-SCNNs/ckpt_epoch_150_AdaBN.pt\n",
      "Loading model: from /Users/codiri/University/Edinburgh/Lightweight-SCNNs/ckpt_epoch_150_CBAM.pt\n",
      "Loaded CBAM model from /Users/codiri/University/Edinburgh/Lightweight-SCNNs/ckpt_epoch_150_CBAM.pt\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Directory to save the models\n",
    "save_dir = os.getcwd()  # Current working directory\n",
    "\n",
    "\n",
    "for name, (model, optim) in baseline_dict.items():\n",
    "\tpath = os.path.join(save_dir, f\"ckpt_epoch_150_{name}.pt\")\n",
    "\tprint(f\"Loading model: from {path}\")\n",
    "\tif os.path.exists(path):\n",
    "\t\tstate_dict = torch.load(path, map_location=device)\n",
    "\t\tmodel.load_state_dict(state_dict[\"model_state_dict\"])\n",
    "\t\tprint(f\"Loaded {name} model from {path}\")\n",
    "\telse:\n",
    "\t\tprint(f\"File not found: {path}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93edcf01",
   "metadata": {},
   "source": [
    "### 3.1) Evaluate model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79cc4e83",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Proto: Accuracy on Cifar10 val set = 15.29%\n",
      "Plain: Accuracy on Cifar10 val set = 24.47%\n",
      "AdaBN: Accuracy on Cifar10 val set = 44.82%\n",
      "CBAM: Accuracy on Cifar10 val set = 24.99%\n"
     ]
    }
   ],
   "source": [
    "# Evaluate total accuracy on val_t_imagenet_loader for all 4 models\n",
    "accuracies = {}\n",
    "\n",
    "for name, (model, optim) in baseline_dict.items():\n",
    "\tmodel.eval()\n",
    "\tcorrect = 0\n",
    "\ttotal = 0\n",
    "\twith torch.no_grad():\n",
    "\t\tfor images, labels in test_loader:\n",
    "\t\t\timages = images.to(device)\n",
    "\t\t\tlabels = labels.to(device)\n",
    "\t\t\tfeats, outputs = model(images)\n",
    "\t\t\t_, preds = outputs.max(dim=1)\n",
    "\t\t\tcorrect += (preds == labels).sum().item()\n",
    "\t\t\ttotal += labels.size(0)\n",
    "\tacc = correct / total\n",
    "\taccuracies[name] = acc\n",
    "\tprint(f\"{name}: Accuracy on Cifar10 val set = {acc*100:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1feced7b",
   "metadata": {},
   "source": [
    "## 4) Evaluate model on CIFAR-10C dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "771cc68d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Evaluating corruption: gaussian_noise (severity=1)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/codiri/installs/miniconda3/envs/SCNN/lib/python3.12/site-packages/torch/utils/data/dataloader.py:683: UserWarning: 'pin_memory' argument is set as true but not supported on MPS now, then device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Corruption = gaussian_noise  | Proto: 14.83%  Plain: 25.94%  AdaBN: 43.44%  CBAM: 23.15%\n",
      "\n",
      "Evaluating corruption: shot_noise (severity=1)\n"
     ]
    }
   ],
   "source": [
    "severity = 1  # Severity level (1-5), where 1 is the least severe\n",
    "batch_size = 256\n",
    "\n",
    "# We'll store per-model, per-corruption accuracies here:\n",
    "results = { name: [] for name in baseline_dict.keys() }\n",
    "\n",
    "# For each corruption type, build a CIFAR10C loader and measure accuracy:\n",
    "for corruption in corruptions:\n",
    "\t# Create the CIFAR-10-C dataset for this corruption & severity\n",
    "\tprint(f\"\\nEvaluating corruption: {corruption} (severity={severity})\")\n",
    "\tds_c = CIFAR10C(\n",
    "\t\tdata_dir=CIFAR10C_ROOT,\n",
    "\t\tcorruption=corruption,\n",
    "\t\tseverity=severity,\n",
    "\t\ttransform=test_transform\n",
    "\t)\n",
    "\tloader_c = DataLoader(ds_c,\n",
    "\t\t\t\t\t\t  batch_size=batch_size,\n",
    "\t\t\t\t\t\t  shuffle=False,\n",
    "\t\t\t\t\t\t  num_workers=2,\n",
    "\t\t\t\t\t\t  pin_memory=True)\n",
    "\n",
    "\t# For each model, run inference on this loader and compute accuracy\n",
    "\tfor name, (model, optim) in baseline_dict.items():\n",
    "\t\t#print(f\"  Evaluating {name}...\", end='\\n')\n",
    "\t\tmodel.eval()  # Set model to evaluation mode\n",
    "\t\tcorrect = 0\n",
    "\t\ttotal = 0\n",
    "\n",
    "\t\twith torch.no_grad():\n",
    "\t\t\tfor images, labels in loader_c:\n",
    "\t\t\t\timages = images.to(device)\n",
    "\t\t\t\tlabels = labels.to(device)\n",
    "\n",
    "\t\t\t\t_, outputs = model(images)               # (B, 10)\n",
    "\t\t\t\t_, preds = outputs.max(dim=1)         # (B,)\n",
    "\t\t\t\tcorrect += (preds == labels).sum().item()\n",
    "\t\t\t\ttotal += labels.size(0)\n",
    "\t\t\t\t#print(f\"  {name}: {correct}/{total} ({100 * correct / total:.2f}%)\", end='\\r')\n",
    "\n",
    "\t\tacc = correct / total\n",
    "\t\tresults[name].append(acc)\n",
    "\n",
    "\tprint(f\"Corruption = {corruption:<15} | \"\n",
    "\t\t  + \"  \".join([f\"{n}: {results[n][-1]*100:5.2f}%\" for n in baseline_dict.keys()]) )\n",
    "\n",
    "# 5) Summarize average across all corruptions\n",
    "# ─────────────────────────────────────────────────────────────────────────────\n",
    "\n",
    "print(\"\\nAverage accuracy over all 15 corruptions (severity=1):\")\n",
    "for name in baseline_dict.keys():\n",
    "\tavg_acc = np.mean(results[name])\n",
    "\tprint(f\"  {name:<5}  →  {avg_acc*100:5.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46281fc1",
   "metadata": {},
   "source": [
    "### 4.1) Load concatenated CIFAR-10C dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6aa1c44",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading CIFAR-10-C corruption arrays...\n",
      "Corruption: gaussian_noise, shape: (50000, 32, 32, 3), dtype: uint8\n",
      "Corruption: shot_noise, shape: (50000, 32, 32, 3), dtype: uint8\n",
      "Corruption: impulse_noise, shape: (50000, 32, 32, 3), dtype: uint8\n",
      "Corruption: defocus_blur, shape: (50000, 32, 32, 3), dtype: uint8\n",
      "Corruption: glass_blur, shape: (50000, 32, 32, 3), dtype: uint8\n",
      "Corruption: motion_blur, shape: (50000, 32, 32, 3), dtype: uint8\n",
      "Corruption: zoom_blur, shape: (50000, 32, 32, 3), dtype: uint8\n",
      "Corruption: snow, shape: (50000, 32, 32, 3), dtype: uint8\n",
      "Corruption: frost, shape: (50000, 32, 32, 3), dtype: uint8\n",
      "Corruption: fog, shape: (50000, 32, 32, 3), dtype: uint8\n",
      "Corruption: brightness, shape: (50000, 32, 32, 3), dtype: uint8\n",
      "Corruption: contrast, shape: (50000, 32, 32, 3), dtype: uint8\n",
      "Corruption: elastic_transform, shape: (50000, 32, 32, 3), dtype: uint8\n",
      "Corruption: pixelate, shape: (50000, 32, 32, 3), dtype: uint8\n",
      "Corruption: jpeg_compression, shape: (50000, 32, 32, 3), dtype: uint8\n",
      "Concatenated array shape: (750000, 32, 32, 3), dtype: uint8\n",
      "Labels shape: (50000,), dtype: uint8\n",
      "All labels shape: (750000,), dtype: uint8\n"
     ]
    }
   ],
   "source": [
    "# Load and inspect CIFAR-10-C corruption arrays\n",
    "print(\"Loading CIFAR-10-C corruption arrays...\")\n",
    "arr = []\n",
    "\n",
    "\n",
    "for corruption in corruptions:\n",
    "\tarr_c = np.load(os.path.join(CIFAR10C_ROOT, f\"{corruption}.npy\"))\n",
    "\tprint(f\"Corruption: {corruption}, shape: {arr_c.shape}, dtype: {arr_c.dtype}\")\n",
    "\n",
    "\t# concatanate all corruption arrays into a single tensor\n",
    "\tarr.append(arr_c)\n",
    "\n",
    "arr = np.concatenate(arr, axis=0)\n",
    "print(f\"Concatenated array shape: {arr.shape}, dtype: {arr.dtype}\")\n",
    "\n",
    "labels = np.load(os.path.join(CIFAR10C_ROOT, \"labels.npy\"))\n",
    "print(f\"Labels shape: {labels.shape}, dtype: {labels.dtype}\")\n",
    "all_labels = np.concatenate([labels] * len(corruptions), axis=0)\n",
    "print(f\"All labels shape: {all_labels.shape}, dtype: {all_labels.dtype}\")\n",
    "\n",
    "# Create a daatset from the concatenated array\n",
    "ds_cifar10c = ImgClassificationDataset(\n",
    "\tdata=arr,\n",
    "\tlabels=all_labels,\n",
    "\ttransform=test_transform\n",
    ")\n",
    "\n",
    "# Create a DataLoader for the CIFAR-10-C dataset\n",
    "loader_cifar10c = DataLoader(\n",
    "\tds_cifar10c,\n",
    "\tbatch_size=batch_size,\n",
    "\tshuffle=False,\n",
    "\tnum_workers=2\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1fa7179",
   "metadata": {},
   "source": [
    "### 4.2) Compute ECE over entire CIFAR-10-C dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4972af11",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Evaluating Proto on CIFAR-10-C dataset...\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "softmax() received an invalid combination of arguments - got (tuple, dim=int), but expected one of:\n * (Tensor input, int dim, torch.dtype dtype = None, *, Tensor out = None)\n * (Tensor input, name dim, *, torch.dtype dtype = None)\n",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mTypeError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[12]\u001b[39m\u001b[32m, line 7\u001b[39m\n\u001b[32m      5\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m name, (model, optim) \u001b[38;5;129;01min\u001b[39;00m baseline_dict.items():\n\u001b[32m      6\u001b[39m \t\u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33mEvaluating \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mname\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m on CIFAR-10-C dataset...\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m----> \u001b[39m\u001b[32m7\u001b[39m \tece, acc = \u001b[43mevaluate_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m      8\u001b[39m \u001b[43m\t\t\t\t   \u001b[49m\u001b[43mloader\u001b[49m\u001b[43m=\u001b[49m\u001b[43mloader_cifar10c\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m      9\u001b[39m \u001b[43m\t\t\t\t   \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     10\u001b[39m \tacc_list.append(acc)\n\u001b[32m     11\u001b[39m \tece_list.append(ece)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/University/Edinburgh/Lightweight-SCNNs/utils/evaluation.py:56\u001b[39m, in \u001b[36mevaluate_model\u001b[39m\u001b[34m(model, loader, device)\u001b[39m\n\u001b[32m     54\u001b[39m labels = labels.to(device)                \u001b[38;5;66;03m# y: (B,)\u001b[39;00m\n\u001b[32m     55\u001b[39m logits = model(images)               \u001b[38;5;66;03m# logits: (B, C)\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m56\u001b[39m probs = \u001b[43mtorch\u001b[49m\u001b[43m.\u001b[49m\u001b[43msoftmax\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlogits\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdim\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m1\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m     57\u001b[39m preds = probs.argmax(dim=\u001b[32m1\u001b[39m)     \u001b[38;5;66;03m# (,)\u001b[39;00m\n\u001b[32m     59\u001b[39m all_probs.append(probs)         \u001b[38;5;66;03m# list of (B, C)\u001b[39;00m\n",
      "\u001b[31mTypeError\u001b[39m: softmax() received an invalid combination of arguments - got (tuple, dim=int), but expected one of:\n * (Tensor input, int dim, torch.dtype dtype = None, *, Tensor out = None)\n * (Tensor input, name dim, *, torch.dtype dtype = None)\n"
     ]
    }
   ],
   "source": [
    "# compute accuracy and ECE for each model on the CIFAR-10-C dataset\n",
    "acc_list = []\n",
    "ece_list = []\n",
    "\n",
    "for name, (model, optim) in baseline_dict.items():\n",
    "\tprint(f\"\\nEvaluating {name} on CIFAR-10-C dataset...\")\n",
    "\tece, acc = evaluate_model(model=model,\n",
    "\t\t\t\t   loader=loader_cifar10c,\n",
    "\t\t\t\t   device=device)\n",
    "\tacc_list.append(acc)\n",
    "\tece_list.append(ece)\n",
    "\tprint(f\"{name} - Accuracy: {acc*100:.2f}%, ECE: {ece:.4f}\")\n",
    "\n",
    "\t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfbe7648",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "SCNN",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
